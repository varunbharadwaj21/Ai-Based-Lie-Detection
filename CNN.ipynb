{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6x6u8-YgfYv",
        "outputId": "00785af2-f445-4d15-e528-76f06a332f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cleaned CSV saved with 1433 clips to:\n",
            "/content/drive/MyDrive/DOLOS_Project/available_clips.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define your clip folders\n",
        "truth_folder = '/content/drive/MyDrive/DOLOS_Project/clips/truth'\n",
        "deception_folder = '/content/drive/MyDrive/DOLOS_Project/clips/deception'\n",
        "\n",
        "# Collect all available clip filenames\n",
        "truth_files = [f.replace('.mp4', '') for f in os.listdir(truth_folder) if f.endswith('.mp4')]\n",
        "deception_files = [f.replace('.mp4', '') for f in os.listdir(deception_folder) if f.endswith('.mp4')]\n",
        "\n",
        "# Build dataframe\n",
        "data = []\n",
        "\n",
        "for f in truth_files:\n",
        "    data.append({'file_name': f, 'label': 'truth'})\n",
        "\n",
        "for f in deception_files:\n",
        "    data.append({'file_name': f, 'label': 'deception'})\n",
        "\n",
        "df_clean = pd.DataFrame(data)\n",
        "\n",
        "# Save new CSV\n",
        "output_path = '/content/drive/MyDrive/DOLOS_Project/available_clips.csv'\n",
        "df_clean.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Cleaned CSV saved with {len(df_clean)} clips to:\\n{output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06RBEMtNizec",
        "outputId": "102e91fc-29a3-43c9-bef1-19ed397c96c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 1433 clips from split_clips.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [13:36<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ All RGB frames extracted and organized by split and label!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# üöÄ DOLOS RGB Frame Extractor using split_clips.csv\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Define Paths\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "clips_path = os.path.join(base_path, \"clips\")\n",
        "frames_base_path = os.path.join(base_path, \"frames\")\n",
        "os.makedirs(frames_base_path, exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# üìÑ Load Clean Split CSV\n",
        "# ===============================\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"/content/split_clips.csv\"))\n",
        "print(f\"‚úÖ Loaded {len(split_df)} clips from split_clips.csv\")\n",
        "\n",
        "# ===============================\n",
        "# üñºÔ∏è Frame Extraction Function\n",
        "# ===============================\n",
        "def extract_frames(clip_path, output_folder, frame_rate=1):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    vidcap = cv2.VideoCapture(clip_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps // frame_rate) if fps >= frame_rate else 1\n",
        "    count, frame_num = 0, 0\n",
        "\n",
        "    while True:\n",
        "        success, frame = vidcap.read()\n",
        "        if not success:\n",
        "            break\n",
        "        if count % interval == 0:\n",
        "            frame_filename = os.path.join(output_folder, f\"frame_{frame_num:04d}.jpg\")\n",
        "            resized = cv2.resize(frame, (224, 224))\n",
        "            cv2.imwrite(frame_filename, resized)\n",
        "            frame_num += 1\n",
        "        count += 1\n",
        "    vidcap.release()\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Extract Frames for Each Clip\n",
        "# ===============================\n",
        "\n",
        "for _, row in tqdm(split_df.iterrows(), total=len(split_df)):\n",
        "    file = row['file_name']\n",
        "    label = row['label'].lower()\n",
        "    split = row['split'].lower()\n",
        "\n",
        "    # Construct video path (clip already exists)\n",
        "    clip_folder = os.path.join(clips_path, label)\n",
        "    clip_path = os.path.join(clip_folder, f\"{file}.mp4\")\n",
        "\n",
        "    if not os.path.exists(clip_path):\n",
        "        print(f\"‚ùå Clip not found: {clip_path}\")\n",
        "        continue\n",
        "\n",
        "    # Output frame folder\n",
        "    out_folder = os.path.join(frames_base_path, split, label, file)\n",
        "    extract_frames(clip_path, out_folder)\n",
        "\n",
        "print(\"\\nüéâ All RGB frames extracted and organized by split and label!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqsNTKIrnuAu",
        "outputId": "678eb137-1f31-4bd0-8267-4d6597eebc07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 1433 clips from split_clips.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [05:02<00:00,  4.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ All audio files extracted and organized by split and label!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# üöÄ DOLOS Audio Extractor from Video Clips (Organized by Train/Val/Test)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Define Paths\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "clips_path = os.path.join(base_path, \"clips\")\n",
        "audio_base_path = os.path.join(base_path, \"audio\")\n",
        "os.makedirs(audio_base_path, exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# üìÑ Load Clean Split CSV\n",
        "# ===============================\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"/content/split_clips.csv\"))\n",
        "print(f\"‚úÖ Loaded {len(split_df)} clips from split_clips.csv\")\n",
        "\n",
        "# ===============================\n",
        "# üéß Audio Extraction Function\n",
        "# ===============================\n",
        "def extract_audio(clip_path, output_audio_path):\n",
        "    os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-i', clip_path,\n",
        "        '-vn',              # no video\n",
        "        '-acodec', 'pcm_s16le',\n",
        "        '-ar', '16000',      # sample rate 16 kHz\n",
        "        '-ac', '1',          # mono audio\n",
        "        output_audio_path\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Extract Audio for Each Clip\n",
        "# ===============================\n",
        "\n",
        "for _, row in tqdm(split_df.iterrows(), total=len(split_df)):\n",
        "    file = row['file_name']\n",
        "    label = row['label'].lower()\n",
        "    split = row['split'].lower()\n",
        "\n",
        "    # Clip path (already exists)\n",
        "    clip_folder = os.path.join(clips_path, label)\n",
        "    clip_path = os.path.join(clip_folder, f\"{file}.mp4\")\n",
        "\n",
        "    if not os.path.exists(clip_path):\n",
        "        print(f\"‚ùå Clip not found: {clip_path}\")\n",
        "        continue\n",
        "\n",
        "    # Output audio path\n",
        "    audio_output = os.path.join(audio_base_path, split, label, f\"{file}.wav\")\n",
        "    extract_audio(clip_path, audio_output)\n",
        "\n",
        "print(\"\\nüéâ All audio files extracted and organized by split and label!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb0CzfsUrPqu",
        "outputId": "49742de2-b06d-47cd-d689-99ba59661b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ACcOAid1_sW",
        "outputId": "baef8b07-9289-4eb0-be1a-e78dacdbfb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wg2Tl1SD2MLx",
        "outputId": "3928fa98-35a2-4e0c-bb76-ea05fff95292"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/DOLOS_Project/'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"/content/drive/MyDrive/DOLOS_Project/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcDINt3N2vB8",
        "outputId": "ecfa3ef4-e313-436d-f100-27f303738439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 1433 clips from split_clips.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1433/1433 [10:17<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ All spectrograms generated and organized by split and label!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# üöÄ DOLOS Spectrogram Generator from Audio Files\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Define Paths\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "audio_path = os.path.join(base_path, \"audio\")\n",
        "spec_base_path = os.path.join(base_path, \"spectrograms\")\n",
        "os.makedirs(spec_base_path, exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# üìÑ Load Clean Split CSV\n",
        "# ===============================\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"/content/split_clips.csv\"))\n",
        "print(f\"‚úÖ Loaded {len(split_df)} clips from split_clips.csv\")\n",
        "\n",
        "# ===============================\n",
        "# üé® Spectrogram Creation Function\n",
        "# ===============================\n",
        "def create_spectrogram(wav_path, output_image_path):\n",
        "    os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "    y, sr = librosa.load(wav_path, sr=16000)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(2.24, 2.24))  # roughly 224x224 when saved\n",
        "    librosa.display.specshow(S_dB, sr=sr, cmap='viridis')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Generate Spectrograms for Each Clip\n",
        "# ===============================\n",
        "for _, row in tqdm(split_df.iterrows(), total=len(split_df)):\n",
        "    file = row['file_name']\n",
        "    label = row['label'].lower()\n",
        "    split = row['split'].lower()\n",
        "\n",
        "    # Path to .wav\n",
        "    wav_folder = os.path.join(audio_path, split, label)\n",
        "    wav_path = os.path.join(wav_folder, f\"{file}.wav\")\n",
        "\n",
        "    if not os.path.exists(wav_path):\n",
        "        print(f\"‚ùå Missing WAV: {wav_path}\")\n",
        "        continue\n",
        "\n",
        "    # Output .png path\n",
        "    spec_output = os.path.join(spec_base_path, split, label, f\"{file}.png\")\n",
        "    create_spectrogram(wav_path, spec_output)\n",
        "\n",
        "print(\"\\nüéâ All spectrograms generated and organized by split and label!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "482citLp2vll",
        "outputId": "a21aa43e-ec0d-4804-b557-d2d445a4050a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 89.1MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [08:40<00:00, 16.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.7265, Train Acc: 0.5653\n",
            "Val   Loss: 0.6751, Val   Acc: 0.5907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:49<00:00,  9.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.3629, Train Acc: 0.8784\n",
            "Val   Loss: 0.6950, Val   Acc: 0.6047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:47<00:00,  9.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.1401, Train Acc: 0.9671\n",
            "Val   Loss: 0.7688, Val   Acc: 0.6512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:47<00:00,  8.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.0594, Train Acc: 0.9900\n",
            "Val   Loss: 0.9084, Val   Acc: 0.5953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:42<00:00,  8.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.0329, Train Acc: 0.9980\n",
            "Val   Loss: 0.8691, Val   Acc: 0.6372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:45<00:00,  8.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.0297, Train Acc: 0.9940\n",
            "Val   Loss: 0.9576, Val   Acc: 0.6465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:44<00:00,  8.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.0382, Train Acc: 0.9920\n",
            "Val   Loss: 0.9614, Val   Acc: 0.5953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:43<00:00,  8.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.0315, Train Acc: 0.9930\n",
            "Val   Loss: 1.0523, Val   Acc: 0.6186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:43<00:00,  8.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.0226, Train Acc: 0.9940\n",
            "Val   Loss: 1.2077, Val   Acc: 0.5907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:45<00:00,  8.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0243, Train Acc: 0.9930\n",
            "Val   Loss: 1.0263, Val   Acc: 0.6512\n",
            "\n",
            " Training complete. Ready for testing or fusion model.\n"
          ]
        }
      ],
      "source": [
        "# üß† Vision Model: CNN Classifier on RGB Frames (DOLOS Project)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Paths and Parameters\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "frames_path = os.path.join(base_path, \"frames\")\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"/content/split_clips.csv\"))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# ===============================\n",
        "# üì¶ Custom Dataset for Frame Folder (1 frame per clip)\n",
        "# ===============================\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, df, split, transform=None):\n",
        "        self.samples = df[df['split'] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.samples.iloc[idx]\n",
        "        file = row['file_name']\n",
        "        label = 0 if row['label'] == 'truth' else 1\n",
        "        frame_path = os.path.join(frames_path, self.split, row['label'], file, \"frame_0000.jpg\")\n",
        "        image = Image.open(frame_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Transforms and DataLoaders\n",
        "# ===============================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = FrameDataset(split_df, split='train', transform=transform)\n",
        "val_ds = FrameDataset(split_df, split='val', transform=transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# üß† CNN Model (ResNet18)\n",
        "# ===============================\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ===============================\n",
        "# üöÇ Training Loop\n",
        "# ===============================\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    for inputs, labels in tqdm(train_dl):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üß™ Validation Loop\n",
        "# ===============================\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dl:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üîÅ Training\n",
        "# ===============================\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss, val_acc = validate()\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n Training complete. Ready for testing or fusion model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkZoJeEf7vOi",
        "outputId": "5fc92d8c-9685-45d4-8b7a-fec536e77027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:45<00:00,  8.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.7140, Train Acc: 0.5563\n",
            "Val   Loss: 0.7107, Val   Acc: 0.5535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:43<00:00,  8.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.2551, Train Acc: 0.9521\n",
            "Val   Loss: 0.7975, Val   Acc: 0.6093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:42<00:00,  8.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.0580, Train Acc: 0.9980\n",
            "Val   Loss: 1.0377, Val   Acc: 0.5628\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:41<00:00,  8.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.0166, Train Acc: 1.0000\n",
            "Val   Loss: 1.0006, Val   Acc: 0.5907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:41<00:00,  8.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.0072, Train Acc: 1.0000\n",
            "Val   Loss: 1.0745, Val   Acc: 0.5349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:40<00:00,  8.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.0054, Train Acc: 1.0000\n",
            "Val   Loss: 1.1427, Val   Acc: 0.5581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:39<00:00,  8.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.0049, Train Acc: 1.0000\n",
            "Val   Loss: 1.1281, Val   Acc: 0.5488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:39<00:00,  8.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.0031, Train Acc: 1.0000\n",
            "Val   Loss: 1.1786, Val   Acc: 0.5674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:40<00:00,  8.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.0018, Train Acc: 1.0000\n",
            "Val   Loss: 1.1942, Val   Acc: 0.5581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:41<00:00,  8.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0014, Train Acc: 1.0000\n",
            "Val   Loss: 1.2142, Val   Acc: 0.5488\n",
            "\n",
            "‚úÖ Training complete. Ready for testing or fusion model.\n"
          ]
        }
      ],
      "source": [
        "# üß† Audio Model: CNN Classifier on Spectrograms (DOLOS Project)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Paths and Parameters\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "specs_path = os.path.join(base_path, \"spectrograms\")\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"split_clips.csv\"))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# ===============================\n",
        "# üì¶ Custom Dataset for Spectrogram Images\n",
        "# ===============================\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, df, split, transform=None):\n",
        "        self.samples = df[df['split'] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.samples.iloc[idx]\n",
        "        file = row['file_name']\n",
        "        label = 0 if row['label'] == 'truth' else 1\n",
        "        spec_path = os.path.join(specs_path, self.split, row['label'], f\"{file}.png\")\n",
        "        image = Image.open(spec_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Transforms and DataLoaders\n",
        "# ===============================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = SpectrogramDataset(split_df, split='train', transform=transform)\n",
        "val_ds = SpectrogramDataset(split_df, split='val', transform=transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# üß† CNN Model (ResNet18)\n",
        "# ===============================\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ===============================\n",
        "# üöÇ Training Loop\n",
        "# ===============================\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    for inputs, labels in tqdm(train_dl):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üß™ Validation Loop\n",
        "# ===============================\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dl:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üîÅ Training\n",
        "# ===============================\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss, val_acc = validate()\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete. Ready for testing or fusion model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRHxRNn3Jn6P",
        "outputId": "9c786e23-df1f-45dc-8e2a-54182da7f1fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 201MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [24:22<00:00, 45.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.6906, Train Acc: 0.5583\n",
            "Val   Loss: 0.6493, Val   Acc: 0.6093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.3271, Train Acc: 0.9272\n",
            "Val   Loss: 0.6771, Val   Acc: 0.6372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.0553, Train Acc: 0.9930\n",
            "Val   Loss: 0.8048, Val   Acc: 0.6093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.0189, Train Acc: 0.9980\n",
            "Val   Loss: 0.9990, Val   Acc: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.0145, Train Acc: 0.9970\n",
            "Val   Loss: 1.0877, Val   Acc: 0.6140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.0684, Train Acc: 0.9791\n",
            "Val   Loss: 1.3109, Val   Acc: 0.5767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.0681, Train Acc: 0.9791\n",
            "Val   Loss: 1.1980, Val   Acc: 0.6372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.1421, Train Acc: 0.9432\n",
            "Val   Loss: 1.5822, Val   Acc: 0.5814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.1136, Train Acc: 0.9631\n",
            "Val   Loss: 1.2125, Val   Acc: 0.6186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:15<00:00,  2.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0604, Train Acc: 0.9791\n",
            "Val   Loss: 1.0499, Val   Acc: 0.6326\n",
            "\n",
            "‚úÖ Training complete. Multimodal fusion model ready!\n"
          ]
        }
      ],
      "source": [
        "# üî• Multimodal Fusion Model: Combine Vision (Frames) + Audio (Spectrograms)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Paths and Parameters\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "frames_path = os.path.join(base_path, \"frames\")\n",
        "specs_path = os.path.join(base_path, \"spectrograms\")\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"split_clips.csv\"))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# ===============================\n",
        "# üì¶ Custom Dataset for Fusion\n",
        "# ===============================\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, df, split, transform=None):\n",
        "        self.samples = df[df['split'] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.samples.iloc[idx]\n",
        "        file = row['file_name']\n",
        "        label = 0 if row['label'] == 'truth' else 1\n",
        "\n",
        "        frame_path = os.path.join(frames_path, self.split, row['label'], file, \"frame_0000.jpg\")\n",
        "        spec_path = os.path.join(specs_path, self.split, row['label'], f\"{file}.png\")\n",
        "\n",
        "        frame = Image.open(frame_path).convert('RGB')\n",
        "        spec = Image.open(spec_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "            spec = self.transform(spec)\n",
        "\n",
        "        return frame, spec, label\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Transforms and DataLoaders\n",
        "# ===============================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = FusionDataset(split_df, split='train', transform=transform)\n",
        "val_ds = FusionDataset(split_df, split='val', transform=transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# üß† Vision Branch (ResNet18)\n",
        "# ===============================\n",
        "vision_model = models.resnet18(pretrained=True)\n",
        "vision_model.fc = nn.Identity()\n",
        "vision_model = vision_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "# üéß Audio Branch (ResNet18)\n",
        "# ===============================\n",
        "audio_model = models.resnet18(pretrained=True)\n",
        "audio_model.fc = nn.Identity()\n",
        "audio_model = audio_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "# üî• Fusion Classifier\n",
        "# ===============================\n",
        "class FusionNet(nn.Module):\n",
        "    def __init__(self, vision_model, audio_model):\n",
        "        super(FusionNet, self).__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.audio_model = audio_model\n",
        "        self.fc1 = nn.Linear(512*2, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, frame, spec):\n",
        "        v_feat = self.vision_model(frame)\n",
        "        a_feat = self.audio_model(spec)\n",
        "        combined = torch.cat((v_feat, a_feat), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = FusionNet(vision_model, audio_model).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ===============================\n",
        "# üöÇ Training Loop\n",
        "# ===============================\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    for frame, spec, labels in tqdm(train_dl):\n",
        "        frame, spec, labels = frame.to(device), spec.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frame, spec)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * frame.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üß™ Validation Loop\n",
        "# ===============================\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for frame, spec, labels in val_dl:\n",
        "            frame, spec, labels = frame.to(device), spec.to(device), labels.to(device)\n",
        "            outputs = model(frame, spec)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * frame.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üîÅ Training\n",
        "# ===============================\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss, val_acc = validate()\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete. Multimodal fusion model ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3G1rMPcvpMh",
        "outputId": "aa3a77c2-d473-493a-f971-3249b4fe72e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:37<00:00, 18.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.6881, Train Acc: 0.5494\n",
            "Val   Loss: 0.6591, Val   Acc: 0.5953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:37<00:00, 18.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.5005, Train Acc: 0.7667\n",
            "Val   Loss: 0.6823, Val   Acc: 0.6047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:39<00:00, 18.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.3138, Train Acc: 0.8804\n",
            "Val   Loss: 0.8750, Val   Acc: 0.6093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:38<00:00, 18.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.2016, Train Acc: 0.9153\n",
            "Val   Loss: 0.9260, Val   Acc: 0.6000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:37<00:00, 18.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.1338, Train Acc: 0.9472\n",
            "Val   Loss: 0.9502, Val   Acc: 0.6372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:41<00:00, 18.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.1296, Train Acc: 0.9511\n",
            "Val   Loss: 1.0885, Val   Acc: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:41<00:00, 18.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.0733, Train Acc: 0.9741\n",
            "Val   Loss: 1.0230, Val   Acc: 0.6605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:35<00:00, 17.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.0505, Train Acc: 0.9831\n",
            "Val   Loss: 1.0849, Val   Acc: 0.6419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:41<00:00, 18.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.0653, Train Acc: 0.9771\n",
            "Val   Loss: 1.2111, Val   Acc: 0.6326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [09:40<00:00, 18.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0926, Train Acc: 0.9591\n",
            "Val   Loss: 1.3183, Val   Acc: 0.6279\n",
            "\n",
            "‚úÖ Training complete. Multimodal fusion model ready!\n"
          ]
        }
      ],
      "source": [
        "# üî• Multimodal Fusion Model: Combine Vision (Frames) + Audio (Spectrograms) with Data Augmentation\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Paths and Parameters\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "frames_path = os.path.join(base_path, \"frames\")\n",
        "specs_path = os.path.join(base_path, \"spectrograms\")\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"split_clips.csv\"))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# ===============================\n",
        "# üì¶ Custom Dataset for Fusion\n",
        "# ===============================\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, df, split, transform=None):\n",
        "        self.samples = df[df['split'] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.samples.iloc[idx]\n",
        "        file = row['file_name']\n",
        "        label = 0 if row['label'] == 'truth' else 1\n",
        "\n",
        "        frame_path = os.path.join(frames_path, self.split, row['label'], file, \"frame_0000.jpg\")\n",
        "        spec_path = os.path.join(specs_path, self.split, row['label'], f\"{file}.png\")\n",
        "\n",
        "        frame = Image.open(frame_path).convert('RGB')\n",
        "        spec = Image.open(spec_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "            spec = self.transform(spec)\n",
        "\n",
        "        return frame, spec, label\n",
        "\n",
        "# ===============================\n",
        "# üîÑ Transforms and DataLoaders (with Augmentation)\n",
        "# ===============================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = FusionDataset(split_df, split='train', transform=train_transform)\n",
        "val_ds = FusionDataset(split_df, split='val', transform=val_transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# üß† Vision Branch (ResNet18)\n",
        "# ===============================\n",
        "vision_model = models.resnet18(pretrained=True)\n",
        "vision_model.fc = nn.Identity()\n",
        "vision_model = vision_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "# üéß Audio Branch (ResNet18)\n",
        "# ===============================\n",
        "audio_model = models.resnet18(pretrained=True)\n",
        "audio_model.fc = nn.Identity()\n",
        "audio_model = audio_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "# üî• Fusion Classifier\n",
        "# ===============================\n",
        "class FusionNet(nn.Module):\n",
        "    def __init__(self, vision_model, audio_model):\n",
        "        super(FusionNet, self).__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.audio_model = audio_model\n",
        "        self.fc1 = nn.Linear(512*2, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, frame, spec):\n",
        "        v_feat = self.vision_model(frame)\n",
        "        a_feat = self.audio_model(spec)\n",
        "        combined = torch.cat((v_feat, a_feat), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = FusionNet(vision_model, audio_model).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ===============================\n",
        "# üöÇ Training Loop\n",
        "# ===============================\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    for frame, spec, labels in tqdm(train_dl):\n",
        "        frame, spec, labels = frame.to(device), spec.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frame, spec)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * frame.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üß™ Validation Loop\n",
        "# ===============================\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for frame, spec, labels in val_dl:\n",
        "            frame, spec, labels = frame.to(device), spec.to(device), labels.to(device)\n",
        "            outputs = model(frame, spec)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * frame.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# üîÅ Training\n",
        "# ===============================\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss, val_acc = validate()\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete. Multimodal fusion model ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG5WhOON2fLG"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GqVArrsVn-y",
        "outputId": "ccd7b34d-d33b-492a-b537-0462c768358c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model saved at /content/drive/MyDrive/DOLOS_Project/fusion_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Save the fusion model\n",
        "save_path = \"/content/drive/MyDrive/DOLOS_Project/fusion_model.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\" Model saved at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "9RTjHt3itI5A",
        "outputId": "0d75c994-6cfb-4b6c-c7f6-219ba9a064a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# üìÇ Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# üìÇ Define paths\n",
        "save_path = \"/content/drive/MyDrive/DOLOS_Project/fusion_model.pth\"\n",
        "\n",
        "# üß† Reload Fusion Model\n",
        "vision_model = models.resnet18(pretrained=True)\n",
        "vision_model.fc = nn.Identity()\n",
        "audio_model = models.resnet18(pretrained=True)\n",
        "audio_model.fc = nn.Identity()\n",
        "\n",
        "model = FusionNet(vision_model, audio_model).to(device)\n",
        "model.load_state_dict(torch.load(save_path))\n",
        "model.eval()\n",
        "\n",
        "# üéØ Evaluate and collect predictions\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for frame, spec, labels in val_dl:\n",
        "        frame, spec, labels = frame.to(device), spec.to(device), labels.to(device)\n",
        "        outputs = model(frame, spec)\n",
        "        preds = outputs.argmax(1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# üßÆ Compute Confusion Matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "labels = ['Truth', 'Deception']\n",
        "\n",
        "# üé® Plot Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Fusion Model')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBkKo4YyUY07"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# üìÇ Input clips folder\n",
        "clips_path = \"/content/drive/MyDrive/DOLOS_Project/clips\"\n",
        "\n",
        "# üìÇ Output frames folder\n",
        "frames_path = \"/content/drive/MyDrive/DOLOS_Project/frames\"\n",
        "os.makedirs(frames_path, exist_ok=True)\n",
        "\n",
        "NUM_FRAMES = 5\n",
        "\n",
        "# Process truth and deception\n",
        "for label in ['truth', 'deception']:\n",
        "    clip_folder = os.path.join(clips_path, label)\n",
        "    output_label_folder = os.path.join(frames_path, label)\n",
        "    os.makedirs(output_label_folder, exist_ok=True)\n",
        "\n",
        "    for clip_file in os.listdir(clip_folder):\n",
        "        if not clip_file.endswith('.mp4'):\n",
        "            continue\n",
        "\n",
        "        clip_name = os.path.splitext(clip_file)[0]\n",
        "        clip_path = os.path.join(clip_folder, clip_file)\n",
        "\n",
        "        output_clip_folder = os.path.join(output_label_folder, clip_name)\n",
        "        os.makedirs(output_clip_folder, exist_ok=True)\n",
        "\n",
        "        cap = cv2.VideoCapture(clip_path)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        selected_frames = np.linspace(0, total_frames-1, NUM_FRAMES).astype(int)\n",
        "\n",
        "        frame_idx = 0\n",
        "        success = True\n",
        "        while success:\n",
        "            success, frame = cap.read()\n",
        "            if frame_idx in selected_frames and success:\n",
        "                save_path = os.path.join(output_clip_folder, f\"frame_{selected_frames.tolist().index(frame_idx):04d}.jpg\")\n",
        "                cv2.imwrite(save_path, frame)\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "\n",
        "print(\"‚úÖ Frame extraction from DOLOS clips complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhnMLVGMNA_b",
        "outputId": "c8d98145-b89b-49dc-ca93-2f7454f96378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [43:49<00:00, 41.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.6811, Train Acc: 0.5603\n",
            "Val   Loss: 0.6351, Val   Acc: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.5030, Train Acc: 0.7537\n",
            "Val   Loss: 0.6699, Val   Acc: 0.6605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:53<00:00,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.3108, Train Acc: 0.8674\n",
            "Val   Loss: 0.8291, Val   Acc: 0.6605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:53<00:00,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.2374, Train Acc: 0.9033\n",
            "Val   Loss: 0.8098, Val   Acc: 0.6791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.1707, Train Acc: 0.9322\n",
            "Val   Loss: 0.9259, Val   Acc: 0.6465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.1475, Train Acc: 0.9402\n",
            "Val   Loss: 0.7393, Val   Acc: 0.7163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.0940, Train Acc: 0.9701\n",
            "Val   Loss: 0.7902, Val   Acc: 0.7116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.1221, Train Acc: 0.9581\n",
            "Val   Loss: 0.9036, Val   Acc: 0.7116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.0907, Train Acc: 0.9671\n",
            "Val   Loss: 1.0943, Val   Acc: 0.7070\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:52<00:00,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.0508, Train Acc: 0.9791\n",
            "Val   Loss: 0.8489, Val   Acc: 0.7256\n",
            "\n",
            " Training complete. Multimodal fusion LSTM model ready!\n"
          ]
        }
      ],
      "source": [
        "# üî• Multimodal Fusion Model with LSTM: Combine Vision (Frames Sequence) + Audio (Spectrograms)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# üìÇ Paths and Parameters\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/DOLOS_Project\"\n",
        "frames_path = os.path.join(base_path, \"frames\")\n",
        "specs_path = os.path.join(base_path, \"spectrograms\")\n",
        "split_df = pd.read_csv(os.path.join(base_path, \"split_clips.csv\"))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16  # Smaller batch size because LSTM requires more memory\n",
        "EPOCHS = 10\n",
        "NUM_FRAMES = 5\n",
        "\n",
        "# ===============================\n",
        "# üì¶ Custom Dataset for Fusion with Multiple Frames\n",
        "# ===============================\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, df, split, transform=None):\n",
        "        self.samples = df[df['split'] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.samples.iloc[idx]\n",
        "        file = row['file_name']\n",
        "        label = 0 if row['label'] == 'truth' else 1\n",
        "\n",
        "        frame_dir = os.path.join(frames_path, self.split, row['label'], file)\n",
        "        spec_path = os.path.join(specs_path, self.split, row['label'], f\"{file}.png\")\n",
        "\n",
        "        frames = []\n",
        "        for i in range(NUM_FRAMES):\n",
        "            frame_path = os.path.join(frame_dir, f\"frame_{i:04d}.jpg\")\n",
        "            if os.path.exists(frame_path):\n",
        "                frame = Image.open(frame_path).convert('RGB')\n",
        "            else:\n",
        "                # If missing, use a blank black image\n",
        "                frame = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
        "\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            frames.append(frame)\n",
        "        frames = torch.stack(frames)\n",
        "\n",
        "        spec = Image.open(spec_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            spec = self.transform(spec)\n",
        "\n",
        "        return frames, spec, label\n",
        "# ===============================\n",
        "# üîÑ Transforms and DataLoaders (with Augmentation)\n",
        "# ===============================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = FusionDataset(split_df, split='train', transform=train_transform)\n",
        "val_ds = FusionDataset(split_df, split='val', transform=val_transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "#  Vision Branch (ResNet18 Feature Extractor)\n",
        "# ===============================\n",
        "vision_model = models.resnet18(pretrained=True)\n",
        "vision_model.fc = nn.Identity()\n",
        "vision_model = vision_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "#  Audio Branch (ResNet18 Feature Extractor)\n",
        "# ===============================\n",
        "audio_model = models.resnet18(pretrained=True)\n",
        "audio_model.fc = nn.Identity()\n",
        "audio_model = audio_model.to(device)\n",
        "\n",
        "# ===============================\n",
        "#  Fusion Classifier with LSTM\n",
        "# ===============================\n",
        "class FusionLSTMNet(nn.Module):\n",
        "    def __init__(self, vision_model, audio_model, hidden_size=256, num_layers=1):\n",
        "        super(FusionLSTMNet, self).__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.audio_model = audio_model\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size + 512, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, frames, spec):\n",
        "        batch_size, seq_len, c, h, w = frames.shape\n",
        "        frames = frames.view(batch_size * seq_len, c, h, w)\n",
        "        frame_features = self.vision_model(frames)\n",
        "        frame_features = frame_features.view(batch_size, seq_len, -1)\n",
        "\n",
        "        _, (hn, _) = self.lstm(frame_features)\n",
        "        lstm_output = hn[-1]\n",
        "\n",
        "        audio_features = self.audio_model(spec)\n",
        "\n",
        "        combined = torch.cat((lstm_output, audio_features), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = FusionLSTMNet(vision_model, audio_model).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ===============================\n",
        "# Training Loop\n",
        "# ===============================\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    for frames, specs, labels in tqdm(train_dl):\n",
        "        frames, specs, labels = frames.to(device), specs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames, specs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# Validation Loop\n",
        "# ===============================\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for frames, specs, labels in val_dl:\n",
        "            frames, specs, labels = frames.to(device), specs.to(device), labels.to(device)\n",
        "            outputs = model(frames, specs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * frames.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)\n",
        "\n",
        "# ===============================\n",
        "# Training\n",
        "# ===============================\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss, val_acc = validate()\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n Training complete. Multimodal fusion LSTM model ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuXXx8OvaMAD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}