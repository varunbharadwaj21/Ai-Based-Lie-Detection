# -*- coding: utf-8 -*-
"""LSTM(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwLfoPd2isfy_3TzN_XY6TGdScqa_l8o
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import pandas as pd
from tqdm import tqdm

# Paths (Local Disk)
base_path ="path"
model_save_path = os.path.join(base_path, "saved_models")
os.makedirs(model_save_path, exist_ok=True)
frames_path = os.path.join(base_path, "frames")
specs_path = os.path.join(base_path, "spectrograms")
split_df = pd.read_csv(os.path.join(base_path, "split_clips.csv"))

# Device setup (Local GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

IMG_SIZE = 224
BATCH_SIZE = 16
EPOCHS = 30
NUM_FRAMES = 5

# Dataset class
class FusionDataset(Dataset):
    def __init__(self, df, split, transform=None):
        self.samples = df[df['split'] == split].reset_index(drop=True)
        self.split = split
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        row = self.samples.iloc[idx]
        file = row['file_name']
        label = 0 if row['label'] == 'truth' else 1

        frame_dir = os.path.join(frames_path, self.split, row['label'], file)
        spec_path = os.path.join(specs_path, self.split, row['label'], f"{file}.png")

        frames = []
        for i in range(NUM_FRAMES):
            frame_path = os.path.join(frame_dir, f"frame_{i:04d}.jpg")
            if os.path.exists(frame_path):
                frame = Image.open(frame_path).convert('RGB')
            else:
                frame = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))

            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        frames = torch.stack(frames)

        spec = Image.open(spec_path).convert('RGB')
        if self.transform:
            spec = self.transform(spec)

        return frames, spec, label

train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3)], p=0.5),
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_ds = FusionDataset(split_df, split='train', transform=train_transform)
val_ds = FusionDataset(split_df, split='val', transform=val_transform)
train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)

vision_model = models.resnet18(pretrained=True)
audio_model = models.resnet18(pretrained=True)

for param in vision_model.parameters():
    param.requires_grad = False
for param in audio_model.parameters():
    param.requires_grad = False
for param in vision_model.layer4.parameters():
    param.requires_grad = True
for param in audio_model.layer4.parameters():
    param.requires_grad = True

vision_model.fc = nn.Identity()
audio_model.fc = nn.Identity()
vision_model = vision_model.to(device)
audio_model = audio_model.to(device)

class FusionLSTMNet(nn.Module):
    def __init__(self, vision_model, audio_model, hidden_size=256, num_layers=1):
        super(FusionLSTMNet, self).__init__()
        self.vision_model = vision_model
        self.audio_model = audio_model
        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size + 512, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, 2)

    def forward(self, frames, spec):
        batch_size, seq_len, c, h, w = frames.shape
        frames = frames.view(batch_size * seq_len, c, h, w)
        frame_features = self.vision_model(frames)
        frame_features = frame_features.view(batch_size, seq_len, -1)

        _, (hn, _) = self.lstm(frame_features)
        lstm_output = hn[-1]

        audio_features = self.audio_model(spec)

        combined = torch.cat((lstm_output, audio_features), dim=1)
        x = self.fc1(combined)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = FusionLSTMNet(vision_model, audio_model).to(device)

# Optimizer, Loss, Scheduler
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# Training loop
def train_epoch():
    model.train()
    running_loss, correct = 0.0, 0
    for frames, specs, labels in tqdm(train_dl):
        frames, specs, labels = frames.to(device), specs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(frames, specs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * frames.size(0)
        correct += (outputs.argmax(1) == labels).sum().item()
    return running_loss / len(train_dl.dataset), correct / len(train_dl.dataset)

def validate():
    model.eval()
    val_loss, correct = 0.0, 0
    with torch.no_grad():
        for frames, specs, labels in val_dl:
            frames, specs, labels = frames.to(device), specs.to(device), labels.to(device)
            outputs = model(frames, specs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * frames.size(0)
            correct += (outputs.argmax(1) == labels).sum().item()
    return val_loss / len(val_dl.dataset), correct / len(val_dl.dataset)

# Training
for epoch in range(EPOCHS):
    train_loss, train_acc = train_epoch()
    val_loss, val_acc = validate()
    scheduler.step()

    print(f"\nEpoch {epoch+1}/{EPOCHS}:")
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    print(f"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}")

    save_path = os.path.join(model_save_path, f"fusion_lstm_local_epoch_{epoch+1}.pth")
    torch.save(model.state_dict(), save_path)
    print(f"Model checkpoint saved at {save_path}")

print("\nTraining complete.")

# Save the fusion model
save_path ="model.pth"
torch.save(model.state_dict(), save_path)
print(f" Model saved at {save_path}")

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Paths
base_path = "path"
model_path = os.path.join(base_path, "saved_models", "fusion_lstm_local_epoch_20.pth")
frames_path = os.path.join(base_path, "frames")
specs_path = os.path.join(base_path, "spectrograms")
split_df = pd.read_csv(os.path.join(base_path, "split_clips.csv"))

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

IMG_SIZE = 224
NUM_FRAMES = 5
BATCH_SIZE = 16

# Dataset class
class FusionDataset(Dataset):
    def __init__(self, df, split, transform=None):
        self.samples = df[df['split'] == split].reset_index(drop=True)
        self.split = split
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        row = self.samples.iloc[idx]
        file = row['file_name']
        label = 0 if row['label'] == 'truth' else 1

        frame_dir = os.path.join(frames_path, self.split, row['label'], file)
        spec_path_img = os.path.join(specs_path, self.split, row['label'], f"{file}.png")

        frames = []
        for i in range(NUM_FRAMES):
            frame_path = os.path.join(frame_dir, f"frame_{i:04d}.jpg")
            if os.path.exists(frame_path):
                frame = Image.open(frame_path).convert('RGB')
            else:
                frame = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))

            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        frames = torch.stack(frames)

        spec = Image.open(spec_path_img).convert('RGB')
        if self.transform:
            spec = self.transform(spec)

        return frames, spec, label

val_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_ds = FusionDataset(split_df, split='val', transform=val_transform)
val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)

# Model
vision_model = models.resnet18(pretrained=False)
audio_model = models.resnet18(pretrained=False)
vision_model.fc = nn.Identity()
audio_model.fc = nn.Identity()

class FusionLSTMNet(nn.Module):
    def __init__(self, vision_model, audio_model, hidden_size=256, num_layers=1):
        super(FusionLSTMNet, self).__init__()
        self.vision_model = vision_model
        self.audio_model = audio_model
        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size + 512, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, 2)

    def forward(self, frames, spec):
        batch_size, seq_len, c, h, w = frames.shape
        frames = frames.view(batch_size * seq_len, c, h, w)
        frame_features = self.vision_model(frames)
        frame_features = frame_features.view(batch_size, seq_len, -1)

        _, (hn, _) = self.lstm(frame_features)
        lstm_output = hn[-1]

        audio_features = self.audio_model(spec)

        combined = torch.cat((lstm_output, audio_features), dim=1)
        x = self.fc1(combined)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = FusionLSTMNet(vision_model, audio_model).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()

# Evaluation
all_preds = []
all_labels = []

with torch.no_grad():
    for frames, specs, labels in tqdm(val_dl):
        frames, specs, labels = frames.to(device), specs.to(device), labels.to(device)
        outputs = model(frames, specs)
        preds = outputs.argmax(dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Classification report
print(classification_report(all_labels, all_preds, target_names=['Truth', 'Deception']))

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Truth', 'Deception'], yticklabels=['Truth', 'Deception'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()